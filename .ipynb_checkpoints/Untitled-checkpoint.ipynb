{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Creating a 8x3x8 autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for another class I'm taking concurrently (Stanford CS 230), I wrote some functions to build a simple neural net from scratch for an early homework assignment. I'm recycling those helper functions I've already made here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y, n_h):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    n_h -- the desired size of the hidden layer\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = n_h #size of hidden layer--for the 8x3x8 encoder, I want this number to be 3.\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    #I'm randomizing my weights to break symmetry but also multiplying it by 0.01 to avoid exploding gradients/make it run faster.\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation function I will be using for all my neurons in my encoder\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = (1+np.exp(-z))**(-1)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Implement Forward Propagation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = sigmoid(Z1) #sigmoid activation function for Z1\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2) #sigmoid activation function for Z2\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use cross-entropy for this task because I want my values to be as close to 0 or 1 as possible, and cross-entropy tends to maximize the odds of values being pushed to either of these extremes, unlike MSE which would equally allow for a distribution between 0 and 1. In this way, I'm wanting my output to look more like a classification task (where the outputs are either 0 or 1) rather than a regression task (where my outputs can RANGE between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost function (cross-entropy)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cost function\n",
    "    \n",
    "    #cost = np.square(np.subtract(A2,Y)).mean() -- I wrote this in case I decide to change to MSE later on\n",
    "    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),(1-Y))\n",
    "    cost = -(1/m)*np.sum(logprobs) \n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                     \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (2, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m)*np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m)*np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    dZ1 = np.dot(W2.T,dZ2)*(A1*(1-A1)) #note: the (A1*(1-A1)) is the gradient/derivative for the sigmoid activation function\n",
    "    dW1 = (1/m)*np.dot(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.15):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    n_x = layer_sizes(X, Y, n_h)[0]\n",
    "    n_y = layer_sizes(X, Y, n_h)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "\n",
    "        # Forward propagation\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function.\n",
    "        cost = compute_cost(A2, Y)\n",
    " \n",
    "        # Backpropagation.\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update.\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes outputs using forward propagation\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = A2 #note that A2 equals the outpuit\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22199317, 0.87073231, 0.20671916, ..., 0.86960068, 0.31039373,\n",
       "        0.79059309],\n",
       "       [0.71624615, 0.64380983, 0.03245853, ..., 0.49029671, 0.52621009,\n",
       "        0.40343823],\n",
       "       [0.78396039, 0.45898864, 0.61811338, ..., 0.42298102, 0.73225143,\n",
       "        0.56095315],\n",
       "       ...,\n",
       "       [0.43804885, 0.47199715, 0.61046446, ..., 0.8097037 , 0.48905981,\n",
       "        0.32760345],\n",
       "       [0.71350345, 0.71421727, 0.31524009, ..., 0.36112389, 0.899994  ,\n",
       "        0.1123772 ],\n",
       "       [0.85540685, 0.72970446, 0.99803679, ..., 0.4032995 , 0.87013078,\n",
       "        0.06819341]])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "autoencoder_X = np.random.rand(8, 1000)\n",
    "autoencoder_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 1, 0, 1],\n",
       "       [0, 0, 1, ..., 1, 1, 0],\n",
       "       [1, 0, 0, ..., 1, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "autoencoder_X_int = np.random.randint(0, 2, size=(8, 1000))\n",
    "autoencoder_X_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "test_rand = autoencoder_X[:,0].reshape(8,1)\n",
    "test_int = autoencoder_X_int[:,0].reshape(8,1)\n",
    "print(test_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.545267\n",
      "Cost after iteration 1000: 3.930433\n",
      "Cost after iteration 2000: 3.632015\n",
      "Cost after iteration 3000: 3.501390\n",
      "Cost after iteration 4000: 3.382141\n",
      "Cost after iteration 5000: 3.304842\n",
      "Cost after iteration 6000: 3.259207\n",
      "Cost after iteration 7000: 3.226562\n",
      "Cost after iteration 8000: 3.201052\n",
      "Cost after iteration 9000: 3.180103\n",
      "Cost after iteration 10000: 3.162339\n",
      "Cost after iteration 11000: 3.146936\n",
      "Cost after iteration 12000: 3.133357\n",
      "Cost after iteration 13000: 3.121232\n",
      "Cost after iteration 14000: 3.110295\n",
      "Cost after iteration 15000: 3.100346\n",
      "Cost after iteration 16000: 3.091231\n",
      "Cost after iteration 17000: 3.082829\n",
      "Cost after iteration 18000: 3.075044\n",
      "Cost after iteration 19000: 3.067797\n",
      "Cost after iteration 20000: 3.061024\n",
      "Cost after iteration 21000: 3.054671\n",
      "Cost after iteration 22000: 3.048692\n",
      "Cost after iteration 23000: 3.043049\n",
      "Cost after iteration 24000: 3.037710\n",
      "Cost after iteration 25000: 3.032645\n",
      "Cost after iteration 26000: 3.027830\n",
      "Cost after iteration 27000: 3.023244\n",
      "Cost after iteration 28000: 3.018866\n",
      "Cost after iteration 29000: 3.014682\n",
      "Cost after iteration 30000: 3.010675\n",
      "Cost after iteration 31000: 3.006833\n",
      "Cost after iteration 32000: 3.003144\n",
      "Cost after iteration 33000: 2.999596\n",
      "Cost after iteration 34000: 2.996181\n",
      "Cost after iteration 35000: 2.992890\n",
      "Cost after iteration 36000: 2.989714\n",
      "Cost after iteration 37000: 2.986647\n",
      "Cost after iteration 38000: 2.983683\n",
      "Cost after iteration 39000: 2.980814\n",
      "Cost after iteration 40000: 2.978036\n",
      "Cost after iteration 41000: 2.975344\n",
      "Cost after iteration 42000: 2.972732\n",
      "Cost after iteration 43000: 2.970198\n",
      "Cost after iteration 44000: 2.967735\n",
      "Cost after iteration 45000: 2.965342\n",
      "Cost after iteration 46000: 2.963015\n",
      "Cost after iteration 47000: 2.960749\n",
      "Cost after iteration 48000: 2.958543\n",
      "Cost after iteration 49000: 2.956394\n",
      "Cost after iteration 50000: 2.954299\n",
      "Cost after iteration 51000: 2.952256\n",
      "Cost after iteration 52000: 2.950262\n",
      "Cost after iteration 53000: 2.948315\n",
      "Cost after iteration 54000: 2.946413\n",
      "Cost after iteration 55000: 2.944555\n",
      "Cost after iteration 56000: 2.942739\n",
      "Cost after iteration 57000: 2.940962\n",
      "Cost after iteration 58000: 2.939224\n",
      "Cost after iteration 59000: 2.937523\n",
      "Cost after iteration 60000: 2.935858\n",
      "Cost after iteration 61000: 2.934227\n",
      "Cost after iteration 62000: 2.932628\n",
      "Cost after iteration 63000: 2.931062\n",
      "Cost after iteration 64000: 2.929526\n",
      "Cost after iteration 65000: 2.928020\n",
      "Cost after iteration 66000: 2.926542\n",
      "Cost after iteration 67000: 2.925092\n",
      "Cost after iteration 68000: 2.923668\n",
      "Cost after iteration 69000: 2.922271\n",
      "Cost after iteration 70000: 2.920898\n",
      "Cost after iteration 71000: 2.919550\n",
      "Cost after iteration 72000: 2.918225\n",
      "Cost after iteration 73000: 2.916922\n",
      "Cost after iteration 74000: 2.915642\n",
      "Cost after iteration 75000: 2.914382\n",
      "Cost after iteration 76000: 2.913144\n",
      "Cost after iteration 77000: 2.911926\n",
      "Cost after iteration 78000: 2.910727\n",
      "Cost after iteration 79000: 2.909547\n",
      "Cost after iteration 80000: 2.908385\n",
      "Cost after iteration 81000: 2.907241\n",
      "Cost after iteration 82000: 2.906115\n",
      "Cost after iteration 83000: 2.905006\n",
      "Cost after iteration 84000: 2.903913\n",
      "Cost after iteration 85000: 2.902836\n",
      "Cost after iteration 86000: 2.901774\n",
      "Cost after iteration 87000: 2.900728\n",
      "Cost after iteration 88000: 2.899697\n",
      "Cost after iteration 89000: 2.898680\n",
      "Cost after iteration 90000: 2.898364\n",
      "Cost after iteration 91000: 2.897577\n",
      "Cost after iteration 92000: 2.896806\n",
      "Cost after iteration 93000: 2.896055\n",
      "Cost after iteration 94000: 2.895317\n",
      "Cost after iteration 95000: 2.894587\n",
      "Cost after iteration 96000: 2.893865\n",
      "Cost after iteration 97000: 2.893150\n",
      "Cost after iteration 98000: 2.892444\n",
      "Cost after iteration 99000: 2.891745\n"
     ]
    }
   ],
   "source": [
    "optimized_parameters = nn_model(autoencoder_X_int, autoencoder_X_int, 3, 100000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 7.07619441e+00, -9.13235516e-03, -6.47971534e-02,\n",
      "         1.34390741e-02,  6.79145303e-02, -4.85088194e-03,\n",
      "         9.09512203e-01, -7.37835063e+00],\n",
      "       [-1.14691839e+00, -6.57791451e-03, -3.96337231e-02,\n",
      "         2.37822147e-03,  2.62621953e-02,  4.55086420e-03,\n",
      "         7.38520900e+00, -5.94275058e+00],\n",
      "       [ 7.75069038e-02, -7.14096854e-03, -7.45956837e+00,\n",
      "         9.87790109e-03,  7.02262898e+00,  7.85692834e-03,\n",
      "         1.23038668e-01, -1.40365145e-01]]), 'b1': array([[-0.35918334],\n",
      "       [-0.2174124 ],\n",
      "       [-0.02831873]]), 'W2': array([[ 4.29575859e+01, -2.13729761e+01, -9.77356028e-01],\n",
      "       [-1.58072524e-01, -1.21265461e-01, -4.83419114e-01],\n",
      "       [ 2.44787762e+00,  1.65193668e+00, -5.00884927e+01],\n",
      "       [ 2.94672384e-01,  1.38522485e-02,  1.40554973e-01],\n",
      "       [ 6.10742488e-02, -3.90420042e-01,  5.65797499e+00],\n",
      "       [-1.22415188e-01,  1.26887324e-01,  9.90605914e-02],\n",
      "       [ 1.15298544e+01,  3.91675568e+01, -9.21760627e-01],\n",
      "       [-6.87458081e+00, -2.82315911e+00,  5.16337969e-01]]), 'b2': array([[-10.15687921],\n",
      "       [  0.41338507],\n",
      "       [ 19.96561822],\n",
      "       [ -0.19412216],\n",
      "       [ -2.42316586],\n",
      "       [ -0.089142  ],\n",
      "       [-24.04029914],\n",
      "       [  4.44718322]])}\n"
     ]
    }
   ],
   "source": [
    "print(optimized_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99998944]\n",
      " [0.5334759 ]\n",
      " [1.        ]\n",
      " [0.52854655]\n",
      " [0.06021715]\n",
      " [0.47879817]\n",
      " [1.        ]\n",
      " [0.00527484]]\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "answers = predict(optimized_parameters, test_int)\n",
    "print(answers)\n",
    "print(test_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this single example where I'm just comparing by eye the \"real\" answer and the \"autoencoder\" answer, I am getting fairly OK answers (1s are indeed 1s and 0s are 0.54 or below) after 100,000 epochs and a learning rate of 0.15. However, to really determine how good this is, I need to do a more robust evaluation metric. As a result, I will now implement cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are attempts to preprocess my positive training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACATCCGTGCACCTCCG'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathway = os.path.join('./data', 'rap1-lieb-positives.txt')\n",
    "train = np.loadtxt(os.path.abspath(pathway), dtype=str)\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "###one-hot encoding all training examples and flattening into matrix of shape (length of sequence*length of onehot vector, m)\n",
    "\n",
    "\n",
    "# define universe of possible input values\n",
    "alphabet = 'ACTG'\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "\n",
    "#X = np.zeros((17*4, len(train)))\n",
    "X = []\n",
    "for sequence in train:\n",
    "    # integer encode input data\n",
    "    integer_encoding = [char_to_int[char] for char in sequence]\n",
    "    # one hot encode\n",
    "    onehot_encoding = []\n",
    "    for value in integer_encoding:\n",
    "        letter = [0 for _ in range(len(alphabet))]\n",
    "        letter[value] = 1\n",
    "        onehot_encoding.append(letter)\n",
    "    final_onehot = list(itertools.chain(*onehot_encoding))\n",
    "    X.append(final_onehot)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 1]\n",
      " [1 0 0 ... 1 0 0]\n",
      " [0 1 0 ... 1 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 1 0]\n",
      " [1 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 68)\n",
      "(137,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X[:,5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X #for an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parameters = nn_model(X, Y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-1.19871719, -0.44568733,  1.2027864 , -3.33359988, -2.0197612 ,\n",
       "         -1.23423178, -0.70492671, -0.73472676, -2.06623434, -1.18367359,\n",
       "         -0.21156829, -1.17050076, -2.0448145 , -0.08808805, -1.84384248,\n",
       "         -2.09977152,  1.47748143, -0.86352517,  0.56085262, -2.12373595,\n",
       "         -1.12049616, -1.25999124, -2.54334989, -1.29816183, -1.89706703,\n",
       "         -1.43421922, -0.6155915 , -1.5310607 , -2.27325765, -2.16520337,\n",
       "         -1.89962733, -0.62104   , -2.02756589, -1.46971935, -1.41671677,\n",
       "         -2.10429764, -1.61042139, -0.30264605, -1.36616325, -0.35317211,\n",
       "         -0.61669504, -2.88817411, -1.4931996 ,  0.09320683, -2.1310597 ,\n",
       "         -0.40612378, -2.27618685,  0.11616285, -1.03366195, -2.3141491 ,\n",
       "          0.05184475,  0.72986834,  0.46329423, -0.60871242, -0.47629112,\n",
       "          0.10928639, -1.20460324, -2.27824343, -1.32449251,  1.73513968,\n",
       "         -2.369514  , -2.15191996, -0.1621025 , -1.6994323 , -1.51453577,\n",
       "         -2.26549247, -2.5499838 , -2.70235097, -0.49704264, -2.0681518 ,\n",
       "         -1.58972327, -1.03663447, -1.06760158, -1.10859784, -1.30943548,\n",
       "         -0.91147885, -1.28192487, -1.77923275, -1.23624369,  0.0389454 ,\n",
       "         -1.51153763, -2.63743691, -0.27882319, -0.65458366, -1.44147472,\n",
       "         -1.20155192, -2.85717869,  0.12742845, -1.08128497, -1.33095347,\n",
       "          0.63149433, -2.32897729, -1.38134878, -2.29531123, -2.68611185,\n",
       "         -0.61702813,  0.60546214, -3.00348283, -2.14441448, -2.76914462,\n",
       "         -0.61255779, -1.41150759, -0.2606919 , -0.25479383, -0.09994225,\n",
       "         -0.25476457, -0.18209213, -2.1247348 , -2.08505254, -0.91578983,\n",
       "         -0.72811139, -1.66095398, -0.58750724,  0.82011971, -1.48691822,\n",
       "         -1.17722476, -2.00031122, -0.88129727,  0.98560928, -2.71602975,\n",
       "         -2.59696362, -0.75216829, -1.54934844, -1.07251285, -2.67431265,\n",
       "         -1.06033175, -1.26512477, -1.69208516,  0.19294398, -0.07307286,\n",
       "         -0.68414002, -0.54693388, -0.75920214, -0.67803569, -1.77974631,\n",
       "         -2.0585885 , -2.54590515],\n",
       "        [-2.37257786, -1.83830015, -1.88060804, -3.10694589, -2.68538578,\n",
       "         -0.43792551, -2.16580849, -1.68864876, -1.74272453, -2.83997543,\n",
       "         -2.01237845, -1.08419379, -1.78761511, -1.2484336 , -2.77386608,\n",
       "         -2.72797645, -0.38349683, -1.60251458, -1.75720847, -2.64900904,\n",
       "         -1.98428967, -2.15417657, -2.7923982 , -2.15160424, -2.47651918,\n",
       "         -2.87943477, -1.75018838, -2.846062  , -2.92986757, -2.14387104,\n",
       "         -2.38805223, -1.82538388, -3.65851534, -2.15498022, -2.16240755,\n",
       "         -1.25684512, -2.13779309, -2.6858088 , -2.22927226, -2.20502174,\n",
       "         -2.55583755, -3.00832109, -2.04421052, -1.00102097, -2.32066932,\n",
       "         -1.97630158, -1.94133365, -1.73143478, -2.39352732, -2.26279446,\n",
       "         -2.66933536, -0.65807571, -0.11390002, -1.97689838, -2.03694662,\n",
       "         -1.3226094 , -2.37923587, -3.04341794, -1.92913139, -0.44946927,\n",
       "         -1.97313235, -2.05056525,  0.28121647, -1.96720956, -2.43474861,\n",
       "         -2.71835531, -3.32870744, -2.7583468 , -0.93187438, -2.91615686,\n",
       "         -1.94579997, -0.57716638, -2.73689025, -2.52818108, -2.2420396 ,\n",
       "          0.20585197, -2.44914694, -0.69398906, -0.90297952, -2.00568929,\n",
       "         -2.61951572, -2.23806192, -1.83133675, -2.12625636, -2.75242946,\n",
       "         -1.22390849, -2.79973842, -1.6824032 , -2.35779732, -1.52322725,\n",
       "         -0.71593134, -2.29928664, -1.88754524, -2.37501558, -2.76595541,\n",
       "         -1.74890476, -1.16962043, -2.69550834, -2.64811309, -3.32069004,\n",
       "         -1.62267517, -2.79307626, -2.08831601, -1.53028848, -1.20199476,\n",
       "         -0.95650308, -2.29194358, -2.25561941, -2.74904046, -2.62275854,\n",
       "         -1.21522791, -1.97156275, -2.55024193, -1.14965924, -1.70573348,\n",
       "         -1.62193364, -2.82706474, -2.33168846, -1.73260866, -2.54217465,\n",
       "         -2.88570593, -2.62220808, -1.88702371, -0.86820877, -2.34920988,\n",
       "         -2.31828931, -2.28915591, -2.32819597, -1.89435233, -1.84602585,\n",
       "         -1.8139711 , -1.99263952, -0.52696576, -2.83580072, -2.3946894 ,\n",
       "         -2.44399707, -3.11350323],\n",
       "        [-2.28296636, -2.35947402, -1.92451798, -3.02247253, -2.63682853,\n",
       "         -1.75787659, -1.91331561, -2.03162958, -2.21113412, -2.53605086,\n",
       "         -1.88860449, -2.62026989, -2.30354915, -2.51511414, -2.67347601,\n",
       "         -2.78819655, -1.8202895 , -2.44785429, -2.46285457, -2.50097958,\n",
       "         -1.95269366, -2.1513878 , -2.60959014, -2.28785334, -2.5424621 ,\n",
       "         -2.6376545 , -1.87922832, -2.5871632 , -2.74898896, -2.85238885,\n",
       "         -2.46946489, -2.27383766, -2.75410174, -2.25573145, -2.38363868,\n",
       "         -2.83588592, -2.35399422, -2.80349619, -2.21410708, -2.31667358,\n",
       "         -2.26206603, -2.62546303, -2.49775105, -1.84635913, -2.52610791,\n",
       "         -2.27416939, -2.43846662, -2.45114177, -2.28671052, -2.34819497,\n",
       "         -2.6999499 , -2.00839872, -1.79200446, -2.35933073, -2.27170937,\n",
       "         -2.27096233, -2.54108542, -2.86710051, -2.6157287 , -1.79282387,\n",
       "         -2.52090714, -2.34688916, -2.1621361 , -2.37459908, -2.26576149,\n",
       "         -2.61867135, -2.93984644, -2.83970026, -2.38826034, -3.06843169,\n",
       "         -2.15128065, -1.61821731, -2.37812726, -2.38202378, -2.65617947,\n",
       "         -1.472515  , -2.22178617, -1.98072173, -2.56140813, -1.93599214,\n",
       "         -2.52531522, -2.6508976 , -2.35933715, -2.34146022, -2.47480566,\n",
       "         -2.44220441, -2.57981753, -1.48263468, -2.5368864 , -2.39776004,\n",
       "         -1.98892635, -2.44736872, -2.68440339, -2.58657829, -2.72436053,\n",
       "         -2.21634754, -2.13941989, -2.80094038, -2.55796503, -3.02183819,\n",
       "         -2.17165816, -2.55330584, -2.17573078, -2.1468368 , -2.73629553,\n",
       "         -1.46780788, -2.37324576, -2.34474024, -2.74008907, -2.58303207,\n",
       "         -2.48445216, -2.49060861, -2.51249522, -1.99796361, -2.17184724,\n",
       "         -2.75420813, -2.76326963, -2.34606226, -1.93757261, -2.48766687,\n",
       "         -2.86465714, -2.39730162, -2.26360489, -2.0187838 , -2.71011726,\n",
       "         -2.21822819, -2.46784034, -2.71402821, -2.15536083, -1.82174725,\n",
       "         -2.14147846, -2.2156049 , -1.72171349, -2.2866956 , -2.9253838 ,\n",
       "         -2.78124808, -2.85715927]]), 'b1': array([[16.8369606 ],\n",
       "        [12.46556164],\n",
       "        [ 7.53252149]]), 'W2': array([[ -1.96951973,  -6.28355027,  -2.37422954],\n",
       "        [ -0.95100244,   0.16183041,  -7.31159202],\n",
       "        [ -0.26580486,  -9.02845041,  -2.57121348],\n",
       "        [ -9.64870414,  -2.51727321,  -1.48211941],\n",
       "        [ -2.11473878,  -6.15929661,  -2.33091487],\n",
       "        [-11.76843117,  13.95203191, -10.00855901],\n",
       "        [ -2.09403133,  -5.97242632,  -2.48488035],\n",
       "        [ -1.10080172,  -7.66732797,  -2.5967711 ],\n",
       "        [ -9.66414873,  -1.96410645,  -1.653526  ],\n",
       "        [ -0.30435878,  -9.06677718,  -2.49354161],\n",
       "        [ -0.43764873,  -8.68822316,  -2.53857914],\n",
       "        [ -3.2176367 ,   2.01816059,  -2.76494536],\n",
       "        [ -3.09834905,   2.1966509 ,  -3.33565483],\n",
       "        [ -1.64938302,   0.64269523,  -6.10919575],\n",
       "        [ -2.66964564,  -5.50284157,  -2.28178066],\n",
       "        [ -2.9543192 ,  -5.1412159 ,  -2.11943543],\n",
       "        [ -1.41918678,   4.0396471 , -14.40329337],\n",
       "        [ -1.94854964,  -2.65465011,  -2.54224668],\n",
       "        [ -1.38138739,   1.88030805, -11.51680164],\n",
       "        [ -3.24388497,  -4.52540584,  -1.90412693],\n",
       "        [ -2.1473189 ,  -5.94088143,  -2.30045648],\n",
       "        [ -3.24762235,  -4.36381239,  -2.11304237],\n",
       "        [ -3.26353568,  -4.8407764 ,  -2.07391372],\n",
       "        [ -3.26574659,  -4.72583138,  -2.26331307],\n",
       "        [ -3.28497615,  -4.61251106,  -2.09644681],\n",
       "        [ -3.08210025,   2.96880446, -10.15153218],\n",
       "        [ -1.08742362,  -7.6566481 ,  -2.66492808],\n",
       "        [ -0.92849637,  -8.02288509,  -2.48975492],\n",
       "        [ -2.98314475,  -5.10208107,  -2.14737607],\n",
       "        [ -5.75064729,   6.59589171, -10.31244662],\n",
       "        [ -3.24743744,  -4.37077269,  -2.11119056],\n",
       "        [ -1.65538126,  -6.87213212,  -2.42287047],\n",
       "        [ -1.94739288,  -6.28808254,  -2.09239872],\n",
       "        [ -1.95980328,  -6.39295973,  -2.38124036],\n",
       "        [ -2.00523564,  -6.28202252,  -2.32102516],\n",
       "        [ -9.83451573,   9.96594862,  -4.76127181],\n",
       "        [ -2.92676429,  -5.02397937,  -2.33985412],\n",
       "        [ -1.55193004,   0.58439403, -10.3018293 ],\n",
       "        [ -1.795796  ,  -6.69800882,  -2.39862707],\n",
       "        [ -0.10352874,  -9.39002903,  -2.53233251],\n",
       "        [ -0.83126801,  -8.16031751,  -2.52924202],\n",
       "        [ -9.64982192,  -2.46947138,  -1.46123138],\n",
       "        [ -5.52934103,   6.3943397 , -11.87465063],\n",
       "        [ -1.95127387,   0.72590051,  -2.78171656],\n",
       "        [ -2.95465347,  -5.12292153,  -2.14809272],\n",
       "        [ -2.15879191,  -5.79149796,  -2.39657909],\n",
       "        [ -9.65659127,  -2.20084337,  -1.56884167],\n",
       "        [ -0.46140264,  -4.78677857,  -2.55266403],\n",
       "        [ -1.27675023,  -7.38794158,  -2.50755821],\n",
       "        [ -9.66100053,  -2.05323082,  -1.62457553],\n",
       "        [ -1.59651976,   0.64789796, -10.34786536],\n",
       "        [ -0.83099586,   1.40677826,  -9.32704175],\n",
       "        [ -1.78382411,   1.84088915,  -4.27679742],\n",
       "        [ -1.64018748,  -6.8952543 ,  -2.39603282],\n",
       "        [ -0.8991452 ,  -8.03684763,  -2.44644846],\n",
       "        [ -1.78784674,  -2.80607376,  -2.71073629],\n",
       "        [ -1.63979748,  -6.92387011,  -2.31169915],\n",
       "        [ -1.92697433,  -6.53647662,  -2.23669639],\n",
       "        [ -1.70861664,  -2.93415849,  -2.62980842],\n",
       "        [  0.25899768,  -1.79861136,  -5.04846698],\n",
       "        [ -9.66236591,  -2.02252856,  -1.63038625],\n",
       "        [ -3.16942558,  -4.85143427,  -2.29717112],\n",
       "        [ -1.88029067,   2.43052227,  -4.82654287],\n",
       "        [ -1.96960722,  -6.27788902,  -2.38612233],\n",
       "        [ -2.14696415,  -5.96258776,  -2.25432621],\n",
       "        [ -3.24606248,  -4.42910708,  -2.03463417],\n",
       "        [ -3.28340747,  -4.68665056,  -2.00404759],\n",
       "        [ -9.65053927,  -2.42758097,  -1.52280107],\n",
       "        [ -3.23573214,   3.24905153,  -8.9641978 ],\n",
       "        [ -5.1080109 ,   5.74026301, -13.14934278],\n",
       "        [ -3.24846333,  -4.33008233,  -2.15133266],\n",
       "        [-11.76810626,  13.95157728, -10.00820548],\n",
       "        [ -1.79446749,  -6.78651067,  -2.33606353],\n",
       "        [ -1.74815905,  -6.82913757,  -2.44343122],\n",
       "        [ -2.35964094,   0.91251881,  -5.90508845],\n",
       "        [-11.77058054,  13.95518526, -10.01132815],\n",
       "        [ -1.27655125,  -7.40323506,  -2.45015597],\n",
       "        [-10.31721631,  10.80147776,  -5.46796697],\n",
       "        [ -9.79407267,   9.73684554,  -4.39161619],\n",
       "        [  0.02120316,  -9.46502004,  -2.59653578],\n",
       "        [ -1.74653011,  -6.69354659,  -2.41778159],\n",
       "        [ -9.65215672,  -2.37815992,  -1.50186142],\n",
       "        [ -1.77065138,  -2.83733729,  -2.65879459],\n",
       "        [ -1.0021308 ,  -7.91705618,  -2.45801896],\n",
       "        [ -1.80942884,  -6.60028521,  -2.36535556],\n",
       "        [ -1.95368424,   0.41393004,  -2.21033496],\n",
       "        [ -9.64701812,  -2.60850777,  -1.39978577],\n",
       "        [ -0.51375794,  -8.47912992,  -2.76579932],\n",
       "        [ -2.67029023,  -5.48719068,  -2.28440204],\n",
       "        [ -2.88374629,  -1.45749936,  -2.62162747],\n",
       "        [ -2.03799806,   3.64800824, -12.96964893],\n",
       "        [ -9.65670439,  -2.19730009,  -1.57009831],\n",
       "        [ -3.13022241,   3.10083513,  -8.86035681],\n",
       "        [ -9.65726101,  -2.18159467,  -1.58726829],\n",
       "        [ -9.65449218,  -2.28849008,  -1.53492632],\n",
       "        [ -1.92867336,  -6.41778346,  -2.56343881],\n",
       "        [ -1.93893672,   1.13496611, -10.6556611 ],\n",
       "        [ -9.65507913,  -2.2561181 ,  -1.57275308],\n",
       "        [ -2.98388913,  -5.06172191,  -2.21262599],\n",
       "        [ -9.64777207,  -2.54554904,  -1.4707022 ],\n",
       "        [ -1.62455948,  -6.86177595,  -2.51466685],\n",
       "        [ -2.1576039 ,  -5.86159094,  -2.2402308 ],\n",
       "        [ -1.84426771,   1.23294572,  -9.18825564],\n",
       "        [ -1.51889096,  -0.18836885,  -5.53932069],\n",
       "        [ -2.64233007,   4.36210918, -14.28167246],\n",
       "        [ -2.09475041,  -5.92841297,  -2.60608167],\n",
       "        [ -1.47868513,  -7.25116624,  -2.35807414],\n",
       "        [ -2.92635322,  -5.04651332,  -2.2906226 ],\n",
       "        [ -2.95485609,  -5.11080271,  -2.17933287],\n",
       "        [ -1.80310057,  -6.69443484,  -2.3164293 ],\n",
       "        [ -3.13156585,   3.10626302,  -8.88691084],\n",
       "        [ -1.7311075 ,  -6.69395118,  -2.45974638],\n",
       "        [ -0.77487655,  -8.31238769,  -2.33746296],\n",
       "        [ -0.80013114,   1.97222935, -11.73649629],\n",
       "        [ -3.24958646,  -4.2827046 ,  -2.22585415],\n",
       "        [ -1.3533347 ,  -0.20786859,  -2.15633122],\n",
       "        [ -2.67692026,   2.39826122,  -9.85927077],\n",
       "        [ -2.158519  ,  -5.80744249,  -2.36029126],\n",
       "        [ -0.37814689,  -8.78831305,  -2.63776425],\n",
       "        [ -9.65718548,  -2.19894707,  -1.53494825],\n",
       "        [ -9.65166067,  -2.38669172,  -1.52973399],\n",
       "        [ -0.97672934,  -7.92404659,  -2.52737261],\n",
       "        [ -3.2656935 ,  -4.7286114 ,  -2.25781984],\n",
       "        [ -3.95470624,   4.07063504,  -5.6510169 ],\n",
       "        [ -9.65510308,  -2.26486448,  -1.5470197 ],\n",
       "        [ -1.7958941 ,  -6.69130469,  -2.41372178],\n",
       "        [ -2.92651661,  -5.03738311,  -2.31217513],\n",
       "        [ -2.66922446,  -5.552003  ,  -2.15495212],\n",
       "        [ -0.33498004,  -1.75459892,  -4.85416719],\n",
       "        [ -1.00149914,  -7.94836671,  -2.51467468],\n",
       "        [ -1.74735579,  -6.63546064,  -2.6006514 ],\n",
       "        [ -1.02648671,  -7.87049588,  -2.40772026],\n",
       "        [-11.77047413,  13.95505365, -10.01126438],\n",
       "        [ -0.44651951,  -8.63516884,  -2.44925366],\n",
       "        [ -2.88016159,  -1.5134639 ,  -2.50852957],\n",
       "        [ -3.98687012,   3.03902898,  -6.60935903],\n",
       "        [ -2.98230357,  -5.14831   ,  -2.07225805]]), 'b2': array([[ 1.49283594e-02],\n",
       "        [-2.80725021e-01],\n",
       "        [-2.97301313e-01],\n",
       "        [ 2.66073030e-01],\n",
       "        [ 3.46385500e-02],\n",
       "        [ 1.26598061e-01],\n",
       "        [ 3.14433725e-02],\n",
       "        [-1.25792833e-01],\n",
       "        [ 2.66084788e-01],\n",
       "        [-2.88337015e-01],\n",
       "        [-2.59517651e-01],\n",
       "        [ 1.23495780e-05],\n",
       "        [-1.35986435e-02],\n",
       "        [-1.42506909e-01],\n",
       "        [ 9.82583629e-02],\n",
       "        [ 1.24924935e-01],\n",
       "        [-3.94087845e-01],\n",
       "        [-6.52045630e-03],\n",
       "        [-2.60521865e-01],\n",
       "        [ 1.47809465e-01],\n",
       "        [ 3.83964782e-02],\n",
       "        [ 1.47870541e-01],\n",
       "        [ 1.49812243e-01],\n",
       "        [ 1.49847641e-01],\n",
       "        [ 1.51122603e-01],\n",
       "        [ 1.55097529e-02],\n",
       "        [-1.28349775e-01],\n",
       "        [-1.58427495e-01],\n",
       "        [ 1.27423540e-01],\n",
       "        [ 1.22861403e-01],\n",
       "        [ 1.47867824e-01],\n",
       "        [-3.11311562e-02],\n",
       "        [ 1.16797858e-02],\n",
       "        [ 1.37129993e-02],\n",
       "        [ 1.99359988e-02],\n",
       "        [ 1.29152870e-01],\n",
       "        [ 1.22309652e-01],\n",
       "        [-1.50532551e-01],\n",
       "        [-9.68950177e-03],\n",
       "        [-3.33723951e-01],\n",
       "        [-1.77773870e-01],\n",
       "        [ 2.66073699e-01],\n",
       "        [ 1.23783226e-01],\n",
       "        [-1.33937471e-01],\n",
       "        [ 1.24931129e-01],\n",
       "        [ 3.96575016e-02],\n",
       "        [ 2.66078815e-01],\n",
       "        [-2.73709189e-01],\n",
       "        [-9.36728455e-02],\n",
       "        [ 2.66082236e-01],\n",
       "        [-1.43523423e-01],\n",
       "        [-3.84148907e-01],\n",
       "        [-2.33193372e-01],\n",
       "        [-3.35326268e-02],\n",
       "        [-1.64252734e-01],\n",
       "        [-3.02461828e-02],\n",
       "        [-3.35447314e-02],\n",
       "        [ 9.32408263e-03],\n",
       "        [-4.21458085e-02],\n",
       "        [-5.34205773e-01],\n",
       "        [ 2.66083450e-01],\n",
       "        [ 1.42621516e-01],\n",
       "        [-2.57365075e-01],\n",
       "        [ 1.49307775e-02],\n",
       "        [ 3.83872581e-02],\n",
       "        [ 1.47845204e-01],\n",
       "        [ 1.51097428e-01],\n",
       "        [ 2.66074323e-01],\n",
       "        [ 1.79957139e-02],\n",
       "        [ 1.23842501e-01],\n",
       "        [ 1.47884814e-01],\n",
       "        [ 1.26597787e-01],\n",
       "        [-9.73360556e-03],\n",
       "        [-1.66533556e-02],\n",
       "        [-1.69198460e-02],\n",
       "        [ 1.26597848e-01],\n",
       "        [-9.36793187e-02],\n",
       "        [ 1.29113454e-01],\n",
       "        [ 1.29410089e-01],\n",
       "        [-3.62056182e-01],\n",
       "        [-1.72142761e-02],\n",
       "        [ 2.66075560e-01],\n",
       "        [-3.28191538e-02],\n",
       "        [-1.44167841e-01],\n",
       "        [-7.84875646e-03],\n",
       "        [-1.28842346e-01],\n",
       "        [ 2.66071711e-01],\n",
       "        [-2.43268358e-01],\n",
       "        [ 9.83006393e-02],\n",
       "        [ 1.00851039e-01],\n",
       "        [-2.22189032e-01],\n",
       "        [ 2.66078908e-01],\n",
       "        [ 9.95788425e-03],\n",
       "        [ 2.66079455e-01],\n",
       "        [ 2.66077349e-01],\n",
       "        [ 9.37138970e-03],\n",
       "        [-9.30469113e-02],\n",
       "        [ 2.66077799e-01],\n",
       "        [ 1.27437165e-01],\n",
       "        [ 2.66072173e-01],\n",
       "        [-3.60906041e-02],\n",
       "        [ 3.96267633e-02],\n",
       "        [-1.19982678e-01],\n",
       "        [-1.34633147e-01],\n",
       "        [-1.42327300e-01],\n",
       "        [ 3.14624046e-02],\n",
       "        [-5.93907031e-02],\n",
       "        [ 1.22301949e-01],\n",
       "        [ 1.24934857e-01],\n",
       "        [-8.55193802e-03],\n",
       "        [ 9.97831192e-03],\n",
       "        [-1.96117441e-02],\n",
       "        [-1.89006281e-01],\n",
       "        [-4.23598645e-01],\n",
       "        [ 1.47903243e-01],\n",
       "        [-2.22048908e-01],\n",
       "        [-2.06043934e-02],\n",
       "        [ 3.96504390e-02],\n",
       "        [-2.72442643e-01],\n",
       "        [ 2.66079335e-01],\n",
       "        [ 2.66075197e-01],\n",
       "        [-1.49135056e-01],\n",
       "        [ 1.49846788e-01],\n",
       "        [ 2.66962023e-02],\n",
       "        [ 2.66077823e-01],\n",
       "        [-9.68623088e-03],\n",
       "        [ 1.22305006e-01],\n",
       "        [ 9.82786704e-02],\n",
       "        [-3.65139542e-01],\n",
       "        [-1.44224538e-01],\n",
       "        [-1.71895519e-02],\n",
       "        [-1.39617706e-01],\n",
       "        [ 1.26597545e-01],\n",
       "        [-2.57732336e-01],\n",
       "        [ 1.00795680e-01],\n",
       "        [ 1.14222325e-01],\n",
       "        [ 1.27407776e-01]])}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(optimized_parameters, X[:,5]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
