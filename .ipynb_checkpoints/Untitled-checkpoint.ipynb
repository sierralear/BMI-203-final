{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Creating a 8x3x8 autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for another class I'm taking concurrently (Stanford CS 230), I wrote some functions to build a simple neural net from scratch for an early homework assignment. I'm recycling those helper functions I've already made here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y, n_h):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    n_h -- the desired size of the hidden layer\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = n_h #size of hidden layer--for the 8x3x8 encoder, I want this number to be 3.\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    #I'm randomizing my weights to break symmetry but also multiplying it by 0.01 to avoid exploding gradients/make it run faster.\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation function I will be using for all my neurons in my encoder\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = (1+np.exp(-z))**(-1)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Implement Forward Propagation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = sigmoid(Z1) #sigmoid activation function for Z1\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2) #sigmoid activation function for Z2\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use cross-entropy for this task because I want my values to be as close to 0 or 1 as possible, and cross-entropy tends to maximize the odds of values being pushed to either of these extremes, unlike MSE which would equally allow for a distribution between 0 and 1. In this way, I'm wanting my output to look more like a classification task (where the outputs are either 0 or 1) rather than a regression task (where my outputs can RANGE between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost function (cross-entropy)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cost function\n",
    "    \n",
    "    #cost = np.square(np.subtract(A2,Y)).mean() -- I wrote this in case I decide to change to MSE later on\n",
    "    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),(1-Y))\n",
    "    cost = -(1/m)*np.sum(logprobs) \n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                     \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (2, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m)*np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m)*np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    dZ1 = np.dot(W2.T,dZ2)*(A1*(1-A1)) #note: the (A1*(1-A1)) is the gradient/derivative for the sigmoid activation function\n",
    "    dW1 = (1/m)*np.dot(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.15):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    n_x = layer_sizes(X, Y, n_h)[0]\n",
    "    n_y = layer_sizes(X, Y, n_h)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "\n",
    "        # Forward propagation\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function.\n",
    "        cost = compute_cost(A2, Y)\n",
    " \n",
    "        # Backpropagation.\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update.\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        # Print the cost every 10000 iterations\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes outputs using forward propagation\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = A2 #note that A2 equals the outpuit\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22199317, 0.87073231, 0.20671916, ..., 0.86960068, 0.31039373,\n",
       "        0.79059309],\n",
       "       [0.71624615, 0.64380983, 0.03245853, ..., 0.49029671, 0.52621009,\n",
       "        0.40343823],\n",
       "       [0.78396039, 0.45898864, 0.61811338, ..., 0.42298102, 0.73225143,\n",
       "        0.56095315],\n",
       "       ...,\n",
       "       [0.43804885, 0.47199715, 0.61046446, ..., 0.8097037 , 0.48905981,\n",
       "        0.32760345],\n",
       "       [0.71350345, 0.71421727, 0.31524009, ..., 0.36112389, 0.899994  ,\n",
       "        0.1123772 ],\n",
       "       [0.85540685, 0.72970446, 0.99803679, ..., 0.4032995 , 0.87013078,\n",
       "        0.06819341]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "autoencoder_X = np.random.rand(8, 1000)\n",
    "autoencoder_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 1, 0, 1],\n",
       "       [0, 0, 1, ..., 1, 1, 0],\n",
       "       [1, 0, 0, ..., 1, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "autoencoder_X_int = np.random.randint(0, 2, size=(8, 1000))\n",
    "autoencoder_X_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "test_rand = autoencoder_X[:,0].reshape(8,1)\n",
    "test_int = autoencoder_X_int[:,0].reshape(8,1)\n",
    "print(test_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.545267\n",
      "Cost after iteration 1000: 3.930433\n",
      "Cost after iteration 2000: 3.632015\n",
      "Cost after iteration 3000: 3.501390\n",
      "Cost after iteration 4000: 3.382141\n",
      "Cost after iteration 5000: 3.304842\n",
      "Cost after iteration 6000: 3.259207\n",
      "Cost after iteration 7000: 3.226562\n",
      "Cost after iteration 8000: 3.201052\n",
      "Cost after iteration 9000: 3.180103\n",
      "Cost after iteration 10000: 3.162339\n",
      "Cost after iteration 11000: 3.146936\n",
      "Cost after iteration 12000: 3.133357\n",
      "Cost after iteration 13000: 3.121232\n",
      "Cost after iteration 14000: 3.110295\n",
      "Cost after iteration 15000: 3.100346\n",
      "Cost after iteration 16000: 3.091231\n",
      "Cost after iteration 17000: 3.082829\n",
      "Cost after iteration 18000: 3.075044\n",
      "Cost after iteration 19000: 3.067797\n",
      "Cost after iteration 20000: 3.061024\n",
      "Cost after iteration 21000: 3.054671\n",
      "Cost after iteration 22000: 3.048692\n",
      "Cost after iteration 23000: 3.043049\n",
      "Cost after iteration 24000: 3.037710\n",
      "Cost after iteration 25000: 3.032645\n",
      "Cost after iteration 26000: 3.027830\n",
      "Cost after iteration 27000: 3.023244\n",
      "Cost after iteration 28000: 3.018866\n",
      "Cost after iteration 29000: 3.014682\n",
      "Cost after iteration 30000: 3.010675\n",
      "Cost after iteration 31000: 3.006833\n",
      "Cost after iteration 32000: 3.003144\n",
      "Cost after iteration 33000: 2.999596\n",
      "Cost after iteration 34000: 2.996181\n",
      "Cost after iteration 35000: 2.992890\n",
      "Cost after iteration 36000: 2.989714\n",
      "Cost after iteration 37000: 2.986647\n",
      "Cost after iteration 38000: 2.983683\n",
      "Cost after iteration 39000: 2.980814\n",
      "Cost after iteration 40000: 2.978036\n",
      "Cost after iteration 41000: 2.975344\n",
      "Cost after iteration 42000: 2.972732\n",
      "Cost after iteration 43000: 2.970198\n",
      "Cost after iteration 44000: 2.967735\n",
      "Cost after iteration 45000: 2.965342\n",
      "Cost after iteration 46000: 2.963015\n",
      "Cost after iteration 47000: 2.960749\n",
      "Cost after iteration 48000: 2.958543\n",
      "Cost after iteration 49000: 2.956394\n",
      "Cost after iteration 50000: 2.954299\n",
      "Cost after iteration 51000: 2.952256\n",
      "Cost after iteration 52000: 2.950262\n",
      "Cost after iteration 53000: 2.948315\n",
      "Cost after iteration 54000: 2.946413\n",
      "Cost after iteration 55000: 2.944555\n",
      "Cost after iteration 56000: 2.942739\n",
      "Cost after iteration 57000: 2.940962\n",
      "Cost after iteration 58000: 2.939224\n",
      "Cost after iteration 59000: 2.937523\n",
      "Cost after iteration 60000: 2.935858\n",
      "Cost after iteration 61000: 2.934227\n",
      "Cost after iteration 62000: 2.932628\n",
      "Cost after iteration 63000: 2.931062\n",
      "Cost after iteration 64000: 2.929526\n",
      "Cost after iteration 65000: 2.928020\n",
      "Cost after iteration 66000: 2.926542\n",
      "Cost after iteration 67000: 2.925092\n",
      "Cost after iteration 68000: 2.923668\n",
      "Cost after iteration 69000: 2.922271\n",
      "Cost after iteration 70000: 2.920898\n",
      "Cost after iteration 71000: 2.919550\n",
      "Cost after iteration 72000: 2.918225\n",
      "Cost after iteration 73000: 2.916922\n",
      "Cost after iteration 74000: 2.915642\n",
      "Cost after iteration 75000: 2.914382\n",
      "Cost after iteration 76000: 2.913144\n",
      "Cost after iteration 77000: 2.911926\n",
      "Cost after iteration 78000: 2.910727\n",
      "Cost after iteration 79000: 2.909547\n",
      "Cost after iteration 80000: 2.908385\n",
      "Cost after iteration 81000: 2.907241\n",
      "Cost after iteration 82000: 2.906115\n",
      "Cost after iteration 83000: 2.905006\n",
      "Cost after iteration 84000: 2.903913\n",
      "Cost after iteration 85000: 2.902836\n",
      "Cost after iteration 86000: 2.901774\n",
      "Cost after iteration 87000: 2.900728\n",
      "Cost after iteration 88000: 2.899697\n",
      "Cost after iteration 89000: 2.898680\n",
      "Cost after iteration 90000: 2.898364\n",
      "Cost after iteration 91000: 2.897577\n",
      "Cost after iteration 92000: 2.896806\n",
      "Cost after iteration 93000: 2.896055\n",
      "Cost after iteration 94000: 2.895317\n",
      "Cost after iteration 95000: 2.894587\n",
      "Cost after iteration 96000: 2.893865\n",
      "Cost after iteration 97000: 2.893150\n",
      "Cost after iteration 98000: 2.892444\n",
      "Cost after iteration 99000: 2.891745\n"
     ]
    }
   ],
   "source": [
    "optimized_parameters = nn_model(autoencoder_X_int, autoencoder_X_int, 3, 100000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 7.07619441e+00, -9.13235516e-03, -6.47971534e-02,\n",
      "         1.34390741e-02,  6.79145303e-02, -4.85088194e-03,\n",
      "         9.09512203e-01, -7.37835063e+00],\n",
      "       [-1.14691839e+00, -6.57791451e-03, -3.96337231e-02,\n",
      "         2.37822147e-03,  2.62621953e-02,  4.55086420e-03,\n",
      "         7.38520900e+00, -5.94275058e+00],\n",
      "       [ 7.75069038e-02, -7.14096854e-03, -7.45956837e+00,\n",
      "         9.87790109e-03,  7.02262898e+00,  7.85692834e-03,\n",
      "         1.23038668e-01, -1.40365145e-01]]), 'b1': array([[-0.35918334],\n",
      "       [-0.2174124 ],\n",
      "       [-0.02831873]]), 'W2': array([[ 4.29575859e+01, -2.13729761e+01, -9.77356028e-01],\n",
      "       [-1.58072524e-01, -1.21265461e-01, -4.83419114e-01],\n",
      "       [ 2.44787762e+00,  1.65193668e+00, -5.00884927e+01],\n",
      "       [ 2.94672384e-01,  1.38522485e-02,  1.40554973e-01],\n",
      "       [ 6.10742488e-02, -3.90420042e-01,  5.65797499e+00],\n",
      "       [-1.22415188e-01,  1.26887324e-01,  9.90605914e-02],\n",
      "       [ 1.15298544e+01,  3.91675568e+01, -9.21760627e-01],\n",
      "       [-6.87458081e+00, -2.82315911e+00,  5.16337969e-01]]), 'b2': array([[-10.15687921],\n",
      "       [  0.41338507],\n",
      "       [ 19.96561822],\n",
      "       [ -0.19412216],\n",
      "       [ -2.42316586],\n",
      "       [ -0.089142  ],\n",
      "       [-24.04029914],\n",
      "       [  4.44718322]])}\n"
     ]
    }
   ],
   "source": [
    "print(optimized_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder thinks the answer is: [[0.99998944]\n",
      " [0.5334759 ]\n",
      " [1.        ]\n",
      " [0.52854655]\n",
      " [0.06021715]\n",
      " [0.47879817]\n",
      " [1.        ]\n",
      " [0.00527484]]\n",
      "The ground-truth is: [[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "answers = predict(optimized_parameters, test_int)\n",
    "print(\"Autoencoder thinks the answer is:\",answers)\n",
    "print(\"The ground-truth is:\",test_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this single example where I'm just comparing by eye the \"real\" answer and the \"autoencoder\" answer, I am getting fairly OK answers (1s are indeed 1s and 0s are 0.54 or below) after 100,000 epochs and a learning rate of 0.15. However, to really determine how good this is, I need to do a more robust evaluation metric. As a result, I will now implement cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, folds=4):\n",
    "    transposed_dataset = dataset.T\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(transposed_dataset)\n",
    "    fold_size = int(len(transposed_dataset) / folds)\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = np.random.randint(0,(len(dataset_copy)))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "        \n",
    "    dataset_split = np.array(dataset_split)\n",
    "    detransposed_split = np.swapaxes(dataset_split, 1, 2)\n",
    "    return detransposed_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(autoencoder_X_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = cross_validation_split(autoencoder_X_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 0 ... 0 1 0]\n",
      "  [0 0 1 ... 1 1 0]\n",
      "  [1 1 0 ... 1 1 0]\n",
      "  ...\n",
      "  [0 0 1 ... 0 1 1]\n",
      "  [0 1 1 ... 1 1 1]\n",
      "  [0 0 0 ... 1 1 0]]\n",
      "\n",
      " [[1 0 0 ... 0 1 1]\n",
      "  [0 0 1 ... 0 0 1]\n",
      "  [0 1 0 ... 0 0 0]\n",
      "  ...\n",
      "  [1 1 0 ... 0 1 1]\n",
      "  [1 0 1 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 1 0 ... 1 0 1]\n",
      "  [0 1 0 ... 1 1 1]\n",
      "  [1 0 0 ... 1 0 1]\n",
      "  ...\n",
      "  [0 0 1 ... 0 0 1]\n",
      "  [0 1 0 ... 0 1 0]\n",
      "  [0 1 1 ... 1 0 1]]\n",
      "\n",
      " [[0 0 1 ... 1 1 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 1 0]\n",
      "  ...\n",
      "  [1 0 0 ... 1 0 1]\n",
      "  [0 1 1 ... 0 0 0]\n",
      "  [0 1 0 ... 1 0 0]]]\n",
      "(4, 8, 250)\n",
      "(8, 250)\n"
     ]
    }
   ],
   "source": [
    "print(folds)\n",
    "print(folds.shape)\n",
    "print(folds[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am looking at shape of all the folds and the first fold and it looks to be correct. I'm seeing an array with 1s and 0s. Additionally, there should be fours folds, each of which should have 8 (number of values per example) x 250 (1000 examples divided by 4 folds).\n",
    "\n",
    "Now, I have to decide on an evaluation metric--for ease, I'm going to pick accuracy, or what percentage of predictions equal my output. Since it's only correct if the prediction equals the ground-truth by the tiniest margin, I lose some nuance, i.e. 0.999 would be counted wrong even though it's basically 1, but I can accept this limitation as it becomes tricky to decide where the threshold/cutoff (i.e. should 0.998 be correct? What about 0.990? What about 0.7?) with a toy autoencoder problem. That said, if I decide later on that I want to adjust the threshold, I can use the np.isclose to pick different margin of errors between the two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage between two lists\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = np.isclose(actual, predicted)\n",
    "    return correct.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric(test_int, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.545587\n",
      "Cost after iteration 10000: 3.112546\n",
      "Cost after iteration 20000: 3.057971\n",
      "Cost after iteration 30000: 3.042753\n",
      "Cost after iteration 40000: 3.029143\n",
      "Cost after iteration 50000: 3.016248\n",
      "Cost after iteration 60000: 3.005780\n",
      "Cost after iteration 70000: 2.997453\n",
      "Cost after iteration 80000: 2.990603\n",
      "Cost after iteration 90000: 2.984816\n",
      "Cost after iteration 0: 5.545197\n",
      "Cost after iteration 10000: 3.164038\n",
      "Cost after iteration 20000: 3.114811\n",
      "Cost after iteration 30000: 3.083270\n",
      "Cost after iteration 40000: 3.059031\n",
      "Cost after iteration 50000: 3.043219\n",
      "Cost after iteration 60000: 3.031552\n",
      "Cost after iteration 70000: 3.022419\n",
      "Cost after iteration 80000: 3.014989\n",
      "Cost after iteration 90000: 3.008773\n",
      "Cost after iteration 0: 5.545027\n",
      "Cost after iteration 10000: 3.145609\n",
      "Cost after iteration 20000: 3.028132\n",
      "Cost after iteration 30000: 2.970224\n",
      "Cost after iteration 40000: 2.933621\n",
      "Cost after iteration 50000: 2.909264\n",
      "Cost after iteration 60000: 2.891445\n",
      "Cost after iteration 70000: 2.877399\n",
      "Cost after iteration 80000: 2.865831\n",
      "Cost after iteration 90000: 2.856029\n",
      "Cost after iteration 0: 5.545272\n",
      "Cost after iteration 10000: 3.151950\n",
      "Cost after iteration 20000: 3.075768\n",
      "Cost after iteration 30000: 3.050369\n",
      "Cost after iteration 40000: 3.032911\n",
      "Cost after iteration 50000: 3.019348\n",
      "Cost after iteration 60000: 3.009005\n",
      "Cost after iteration 70000: 3.000789\n",
      "Cost after iteration 80000: 2.994013\n",
      "Cost after iteration 90000: 2.988283\n",
      "The accuracy on each of the folds is: [8.95, 13.350000000000001, 15.65, 8.75]\n",
      "The mean accuracy of my model is: 11.675\n"
     ]
    }
   ],
   "source": [
    "accuracy_avg = []\n",
    "for i, fold in enumerate(folds): #loop through each fold\n",
    "    training_set = np.delete(folds, i, axis=0).reshape(8, 250*3)\n",
    "    parameters = nn_model(training_set, training_set, 3, 100000, print_cost=True)\n",
    "    predicted = predict(parameters, folds[i])\n",
    "    accuracy = accuracy_metric(folds[i], predicted)\n",
    "    accuracy_avg.append(accuracy)\n",
    "print(\"The accuracy on each of the folds is:\",accuracy_avg)\n",
    "print(\"The mean accuracy of my model is:\",np.mean(accuracy_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.545587\n",
      "[[0.88961599 0.19939553 0.22205292 ... 0.03406584 0.29293257 0.2822857 ]\n",
      " [0.55136543 0.3935668  0.67478609 ... 0.45578419 0.67450037 0.56963796]\n",
      " [0.49593162 0.84757212 0.64077784 ... 0.83248679 0.48716884 0.5510572 ]\n",
      " ...\n",
      " [0.3038918  0.15477646 0.55406023 ... 0.37893795 0.70538138 0.59305043]\n",
      " [0.55775664 0.85622219 0.72787751 ... 0.82599609 0.54020079 0.55458699]\n",
      " [0.04101077 0.31985159 0.24274222 ... 0.83162487 0.4466251  0.58611662]]\n",
      "[[1 0 0 ... 0 1 0]\n",
      " [0 0 1 ... 1 1 0]\n",
      " [1 1 0 ... 1 1 0]\n",
      " ...\n",
      " [0 0 1 ... 0 1 1]\n",
      " [0 1 1 ... 1 1 1]\n",
      " [0 0 0 ... 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "training_set = np.delete(folds, 0, axis=0).reshape(8, 250*3)\n",
    "parameters = nn_model(training_set, training_set, 3, 1000, print_cost=True)\n",
    "predicted = predict(parameters, folds[0])\n",
    "print(predicted)\n",
    "print(folds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 4])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [1, 3, 2, 4]\n",
    "np.delete(test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are attempts to preprocess my positive training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathway = os.path.join('./data', 'rap1-lieb-positives.txt')\n",
    "train = np.loadtxt(os.path.abspath(pathway), dtype=str)\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###one-hot encoding all training examples and flattening into matrix of shape (length of sequence*length of onehot vector, m)\n",
    "\n",
    "\n",
    "# define universe of possible input values\n",
    "alphabet = 'ACTG'\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "\n",
    "#X = np.zeros((17*4, len(train)))\n",
    "X = []\n",
    "for sequence in train:\n",
    "    # integer encode input data\n",
    "    integer_encoding = [char_to_int[char] for char in sequence]\n",
    "    # one hot encode\n",
    "    onehot_encoding = []\n",
    "    for value in integer_encoding:\n",
    "        letter = [0 for _ in range(len(alphabet))]\n",
    "        letter[value] = 1\n",
    "        onehot_encoding.append(letter)\n",
    "    final_onehot = list(itertools.chain(*onehot_encoding))\n",
    "    X.append(final_onehot)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X[:,5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X #for an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parameters = nn_model(X, Y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(optimized_parameters, X[:,5]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
