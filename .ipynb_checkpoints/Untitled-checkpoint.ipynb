{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Creating a 8x3x8 autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for another class I'm taking concurrently (Stanford CS 230), I wrote some functions to build a simple neural net from scratch for an early homework assignment. I'm recycling those helper functions I've already made here below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y, n_h):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    n_h -- the desired size of the hidden layer\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = n_h #size of hidden layer--for the 8x3x8 encoder, I want this number to be 3.\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    #I'm randomizing my weights to break symmetry but also multiplying it by 0.01 to avoid exploding gradients/make it run faster.\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation function I will be using for all my neurons in my encoder\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = (1+np.exp(-z))**(-1)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Implement Forward Propagation\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = sigmoid(Z1) #sigmoid activation function for Z1\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2) #sigmoid activation function for Z2\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose to use cross-entropy for this task because I want my values to be as close to 0 or 1 as possible, and cross-entropy tends to maximize the odds of values being pushed to either of these extremes, unlike MSE which would equally allow for a distribution between 0 and 1. In this way, I'm wanting my output to look more like a classification task (where the outputs are either 0 or 1) rather than a regression task (where my outputs can RANGE between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost function (cross-entropy)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cost function\n",
    "    \n",
    "    #cost = np.square(np.subtract(A2,Y)).mean() -- I wrote this in case I decide to change to MSE later on\n",
    "    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),(1-Y))\n",
    "    cost = -(1/m)*np.sum(logprobs) \n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                     \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (2, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1/m)*np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m)*np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    dZ1 = np.dot(W2.T,dZ2)*(A1*(1-A1)) #note: the (A1*(1-A1)) is the gradient/derivative for the sigmoid activation function\n",
    "    dW1 = (1/m)*np.dot(dZ1,X.T)\n",
    "    db1 = (1/m)*np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.15):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    n_x = layer_sizes(X, Y, n_h)[0]\n",
    "    n_y = layer_sizes(X, Y, n_h)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "\n",
    "        # Forward propagation\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function.\n",
    "        cost = compute_cost(A2, Y)\n",
    " \n",
    "        # Backpropagation.\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update.\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        # Print the cost every 10000 iterations\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes outputs using forward propagation\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = A2 #note that A2 equals the outpuit\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, ..., 0, 1, 0],\n",
       "       [0, 1, 1, ..., 1, 0, 1],\n",
       "       [1, 1, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating my training data\n",
    "np.random.seed(5)\n",
    "autoencoder_X_int = np.random.randint(0, 2, size=(8, 8*1000))\n",
    "autoencoder_X_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 1],\n",
       "       [0, 1, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 1, 1, 0, 1, 1]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating some test data--8x8 array\n",
    "test_int_eight = autoencoder_X_int[:,0:8].reshape(8,8)\n",
    "test_int_eight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.545101\n",
      "Cost after iteration 10000: 3.217346\n",
      "Cost after iteration 20000: 3.112220\n",
      "Cost after iteration 30000: 3.060554\n",
      "Cost after iteration 40000: 3.027232\n",
      "Cost after iteration 50000: 3.003088\n",
      "Cost after iteration 60000: 2.984388\n",
      "Cost after iteration 70000: 2.969258\n",
      "Cost after iteration 80000: 2.956633\n",
      "Cost after iteration 90000: 2.948331\n",
      "Cost after iteration 100000: 2.941061\n",
      "Cost after iteration 110000: 2.934609\n",
      "Cost after iteration 120000: 2.924138\n",
      "Cost after iteration 130000: 2.916043\n",
      "Cost after iteration 140000: 2.923334\n",
      "Cost after iteration 150000: 2.907244\n",
      "Cost after iteration 160000: 2.904628\n",
      "Cost after iteration 170000: 2.901226\n",
      "Cost after iteration 180000: 2.940994\n",
      "Cost after iteration 190000: 2.905080\n"
     ]
    }
   ],
   "source": [
    "#training my autoencoder\n",
    "autoencoder_parameters = nn_model(autoencoder_X_int, autoencoder_X_int, 3, 200000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 7.75787226e+00, -3.54403628e-03, -7.76659363e+00,\n",
      "        -1.45688325e-01, -4.01016134e-03, -6.38731554e-03,\n",
      "         1.10907869e-01,  4.78167448e-01],\n",
      "       [ 3.10879884e-02,  6.65737696e-03, -8.30107411e-02,\n",
      "        -8.20174604e+00,  4.12582955e-03,  1.65968009e-03,\n",
      "         7.73017256e+00,  7.50891035e-02],\n",
      "       [-3.49027045e-01,  2.41996150e-03, -7.14013693e+00,\n",
      "        -9.66672266e-02,  8.87494695e-03,  4.50018907e-03,\n",
      "         8.50513108e-02,  7.86569379e+00]]), 'b1': array([[-0.32207626],\n",
      "       [ 0.04704691],\n",
      "       [-0.26117921]]), 'W2': array([[ 5.91338792e+01, -3.06499324e+00, -1.70361258e+01],\n",
      "       [ 6.53306909e-02,  1.86926768e-01, -1.00041231e-01],\n",
      "       [-6.44029046e+00,  5.15353697e-01, -4.38174194e+00],\n",
      "       [ 1.13787349e+00, -6.19677821e+01,  1.78232703e+00],\n",
      "       [ 1.26632573e-02,  9.96841916e-02,  1.03676464e-01],\n",
      "       [-5.01248074e-02,  1.13325684e-02, -1.39925062e-02],\n",
      "       [ 6.02128937e-02,  4.64111967e+00, -1.94124076e-01],\n",
      "       [ 3.96683237e+00, -2.81304887e+00,  5.69197102e+01]]), 'b2': array([[-18.75799657],\n",
      "       [ -0.11250281],\n",
      "       [  4.92008696],\n",
      "       [ 26.87432471],\n",
      "       [ -0.05155465],\n",
      "       [  0.03370776],\n",
      "       [ -2.11097302],\n",
      "       [-27.48116063]])}\n"
     ]
    }
   ],
   "source": [
    "print(autoencoder_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder thinks the answer is: [[0.987 0.    1.    0.898 0.    0.    0.003 0.881]\n",
      " [0.502 0.472 0.48  0.514 0.457 0.519 0.481 0.514]\n",
      " [0.924 0.993 0.049 0.352 0.911 0.996 0.067 0.354]\n",
      " [0.027 1.    1.    0.    1.    0.    0.024 0.   ]\n",
      " [0.501 0.487 0.499 0.528 0.502 0.512 0.528 0.528]\n",
      " [0.505 0.508 0.495 0.502 0.506 0.511 0.5   0.502]\n",
      " [0.557 0.108 0.108 0.921 0.098 0.926 0.549 0.921]\n",
      " [0.    0.    0.01  0.963 0.998 0.    1.    0.966]]\n",
      "The ground-truth is: [[1 0 1 1 0 0 0 1]\n",
      " [0 1 1 0 0 0 0 1]\n",
      " [1 1 0 1 1 1 0 1]\n",
      " [0 1 1 0 1 0 0 0]\n",
      " [0 1 1 0 0 1 1 0]\n",
      " [1 0 0 0 1 1 1 1]\n",
      " [0 0 0 1 0 1 0 1]\n",
      " [0 0 0 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#random test example taken from training data to play with\n",
    "predicted_eight = predict(autoencoder_parameters, test_int_eight)\n",
    "print(\"Autoencoder thinks the answer is:\",np.around(predicted_eight, decimals=3))\n",
    "print(\"The ground-truth is:\",test_int_eight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#identity matrix, which will be real test of my autoencoder\n",
    "identity_matrix = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                           [0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "print(identity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder thinks the answer is: [[1.    0.049 0.    0.048 0.048 0.047 0.042 0.004]\n",
      " [0.504 0.492 0.495 0.468 0.492 0.492 0.514 0.48 ]\n",
      " [0.058 0.64  0.994 0.654 0.638 0.64  0.635 0.066]\n",
      " [0.028 0.024 0.028 1.    0.025 0.026 0.    0.026]\n",
      " [0.512 0.513 0.499 0.499 0.513 0.513 0.525 0.528]\n",
      " [0.496 0.503 0.51  0.502 0.503 0.503 0.504 0.5  ]\n",
      " [0.572 0.553 0.542 0.103 0.552 0.551 0.922 0.547]\n",
      " [0.007 0.078 0.    0.074 0.085 0.081 0.072 1.   ]]\n",
      "The ground-truth is: [[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "predicted_identity = predict(autoencoder_parameters, identity_matrix)\n",
    "print(\"Autoencoder thinks the answer is:\",np.around(predicted_identity, decimals=3))\n",
    "print(\"The ground-truth is:\",identity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, my neural network is not performing the best--while it is \"kinda\" there, especially on certain rows like the first, it is performing fairly abysmally for most of them. I can think of a couple reasons why this is: \n",
    "\n",
    "1) I need to train for most epochs for convergence\n",
    "\n",
    "2) I need to decrease my learning rate, as my cost is increasing during particular epochs\n",
    "\n",
    "3)  My training data, which has about an equal number of 1s and 0s in each row, is not representative of my test, an identity matrix which only has a single 1 per row (don't think this is it, because also does poorly on my test case with data pulled from my training set)\n",
    "\n",
    "4) My choice of loss function is a poor one. I might want to try MSE instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this single example where I'm just comparing by eye the \"real\" answer and the \"autoencoder\" answer, I am getting fairly OK answers (1s are indeed 1s and 0s are 0.54 or below) after 100,000 epochs and a learning rate of 0.15. However, to really determine how good this is, I need to do a more robust evaluation metric. As a result, I will now implement cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, folds=4):\n",
    "    transposed_dataset = dataset.T\n",
    "    dataset_split = list()\n",
    "    dataset_copy = list(transposed_dataset)\n",
    "    fold_size = int(len(transposed_dataset) / folds)\n",
    "    for i in range(folds):\n",
    "        fold = list()\n",
    "        while len(fold) < fold_size:\n",
    "            index = np.random.randint(0,(len(dataset_copy)))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "        \n",
    "    dataset_split = np.array(dataset_split)\n",
    "    detransposed_split = np.swapaxes(dataset_split, 1, 2)\n",
    "    return detransposed_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(autoencoder_X_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = cross_validation_split(autoencoder_X_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 0 ... 0 1 0]\n",
      "  [0 0 1 ... 1 1 0]\n",
      "  [1 1 0 ... 1 1 0]\n",
      "  ...\n",
      "  [0 0 1 ... 0 1 1]\n",
      "  [0 1 1 ... 1 1 1]\n",
      "  [0 0 0 ... 1 1 0]]\n",
      "\n",
      " [[1 0 0 ... 0 1 1]\n",
      "  [0 0 1 ... 0 0 1]\n",
      "  [0 1 0 ... 0 0 0]\n",
      "  ...\n",
      "  [1 1 0 ... 0 1 1]\n",
      "  [1 0 1 ... 0 0 1]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 1 0 ... 1 0 1]\n",
      "  [0 1 0 ... 1 1 1]\n",
      "  [1 0 0 ... 1 0 1]\n",
      "  ...\n",
      "  [0 0 1 ... 0 0 1]\n",
      "  [0 1 0 ... 0 1 0]\n",
      "  [0 1 1 ... 1 0 1]]\n",
      "\n",
      " [[0 0 1 ... 1 1 0]\n",
      "  [1 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 1 0]\n",
      "  ...\n",
      "  [1 0 0 ... 1 0 1]\n",
      "  [0 1 1 ... 0 0 0]\n",
      "  [0 1 0 ... 1 0 0]]]\n",
      "(4, 8, 250)\n",
      "(8, 250)\n"
     ]
    }
   ],
   "source": [
    "print(folds)\n",
    "print(folds.shape)\n",
    "print(folds[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am looking at shape of all the folds and the first fold and it looks to be correct. I'm seeing an array with 1s and 0s. Additionally, there should be fours folds, each of which should have 8 (number of values per example) x 250 (1000 examples divided by 4 folds).\n",
    "\n",
    "Now, I have to decide on an evaluation metric--for ease, I'm going to pick accuracy, or what percentage of predictions equal my output. Since it's only correct if the prediction equals the ground-truth by the tiniest margin, I lose some nuance, i.e. 0.999 would be counted wrong even though it's basically 1, but I can accept this limitation as it becomes tricky to decide where the threshold/cutoff (i.e. should 0.998 be correct? What about 0.990? What about 0.7?) with a toy autoencoder problem. That said, if I decide later on that I want to adjust the threshold, I can use the np.isclose to pick different margin of errors between the two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage between two lists\n",
    "def accuracy_metric(actual, predicted, threshold=1e-08):\n",
    "    correct = np.isclose(actual, predicted, rtol=threshold, atol=threshold)\n",
    "    return correct.mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric(test_int, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.544928\n",
      "Cost after iteration 10000: 3.148037\n",
      "Cost after iteration 20000: 3.051171\n",
      "Cost after iteration 30000: 3.003092\n",
      "Cost after iteration 40000: 2.972017\n",
      "Cost after iteration 50000: 2.949496\n",
      "Cost after iteration 60000: 2.932061\n",
      "Cost after iteration 70000: 2.917966\n",
      "Cost after iteration 80000: 2.906215\n",
      "Cost after iteration 90000: 2.899255\n",
      "Cost after iteration 0: 5.545022\n",
      "Cost after iteration 10000: 3.108594\n",
      "Cost after iteration 20000: 3.059149\n",
      "Cost after iteration 30000: 3.041684\n",
      "Cost after iteration 40000: 3.027895\n",
      "Cost after iteration 50000: 3.015579\n",
      "Cost after iteration 60000: 3.005281\n",
      "Cost after iteration 70000: 2.996854\n",
      "Cost after iteration 80000: 2.989836\n",
      "Cost after iteration 90000: 2.983875\n",
      "Cost after iteration 0: 5.545303\n",
      "Cost after iteration 10000: 3.173270\n",
      "Cost after iteration 20000: 3.116948\n",
      "Cost after iteration 30000: 3.096189\n",
      "Cost after iteration 40000: 3.084359\n",
      "Cost after iteration 50000: 3.075562\n",
      "Cost after iteration 60000: 3.066309\n",
      "Cost after iteration 70000: 3.053326\n",
      "Cost after iteration 80000: 3.038570\n",
      "Cost after iteration 90000: 3.027890\n",
      "Cost after iteration 0: 5.545409\n",
      "Cost after iteration 10000: 3.225102\n",
      "Cost after iteration 20000: 3.097086\n",
      "Cost after iteration 30000: 3.027493\n",
      "Cost after iteration 40000: 2.983123\n",
      "Cost after iteration 50000: 2.950576\n",
      "Cost after iteration 60000: 2.926632\n",
      "Cost after iteration 70000: 2.909098\n",
      "Cost after iteration 80000: 2.895672\n",
      "Cost after iteration 90000: 2.884716\n",
      "The accuracy on each of the folds is: [15.55, 7.75, 5.35, 12.049999999999999]\n",
      "The mean accuracy of my model is: 10.174999999999999\n"
     ]
    }
   ],
   "source": [
    "accuracy_avg = []\n",
    "for i, fold in enumerate(folds): #loop through each fold\n",
    "    training_set = np.delete(folds, i, axis=0).reshape(8, 250*3) #training set are all the folds except the test fold (fold[i])\n",
    "    parameters = nn_model(training_set, training_set, 3, 100000, print_cost=True) #train NN with the training set\n",
    "    predicted = predict(parameters, folds[i]) #predict your answers for the test fold using your trained NN\n",
    "    accuracy = accuracy_metric(folds[i], predicted) #calculate accuracy\n",
    "    accuracy_avg.append(accuracy) #append accuracy to a list of all the accuracies\n",
    "print(\"The accuracy on each of the folds is:\",accuracy_avg) #print the accuracy for each of the folds\n",
    "print(\"The mean accuracy of my model is:\",np.mean(accuracy_avg)) #calculate average accuracy overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very low accuracy, which I'm not especially surprised by given how much I was expecting perfect matches. In order to be a bit more lenient, I will modify my accuracy metric to consider answers that are off by +/- 0.15 to still be \"correct.\" Additionally, to improve my model slightly (but keeping in mind how long it takes to run), I will run for 120,000 epochs instead of 100,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.544928\n",
      "Cost after iteration 10000: 3.148037\n",
      "Cost after iteration 20000: 3.051171\n",
      "Cost after iteration 30000: 3.003092\n",
      "Cost after iteration 40000: 2.972017\n",
      "Cost after iteration 50000: 2.949496\n",
      "Cost after iteration 60000: 2.932061\n",
      "Cost after iteration 70000: 2.917966\n",
      "Cost after iteration 80000: 2.906215\n",
      "Cost after iteration 90000: 2.899255\n",
      "Cost after iteration 100000: 2.893360\n",
      "Cost after iteration 110000: 2.888209\n",
      "Cost after iteration 0: 5.545022\n",
      "Cost after iteration 10000: 3.108594\n",
      "Cost after iteration 20000: 3.059149\n",
      "Cost after iteration 30000: 3.041684\n",
      "Cost after iteration 40000: 3.027895\n",
      "Cost after iteration 50000: 3.015579\n",
      "Cost after iteration 60000: 3.005281\n",
      "Cost after iteration 70000: 2.996854\n",
      "Cost after iteration 80000: 2.989836\n",
      "Cost after iteration 90000: 2.983875\n",
      "Cost after iteration 100000: 2.978729\n",
      "Cost after iteration 110000: 2.974228\n",
      "Cost after iteration 0: 5.545303\n",
      "Cost after iteration 10000: 3.173270\n",
      "Cost after iteration 20000: 3.116948\n",
      "Cost after iteration 30000: 3.096189\n",
      "Cost after iteration 40000: 3.084359\n",
      "Cost after iteration 50000: 3.075562\n",
      "Cost after iteration 60000: 3.066309\n",
      "Cost after iteration 70000: 3.053326\n",
      "Cost after iteration 80000: 3.038570\n",
      "Cost after iteration 90000: 3.027890\n",
      "Cost after iteration 100000: 3.020045\n",
      "Cost after iteration 110000: 3.013534\n",
      "Cost after iteration 0: 5.545409\n",
      "Cost after iteration 10000: 3.225102\n",
      "Cost after iteration 20000: 3.097086\n",
      "Cost after iteration 30000: 3.027493\n",
      "Cost after iteration 40000: 2.983123\n",
      "Cost after iteration 50000: 2.950576\n",
      "Cost after iteration 60000: 2.926632\n",
      "Cost after iteration 70000: 2.909098\n",
      "Cost after iteration 80000: 2.895672\n",
      "Cost after iteration 90000: 2.884716\n",
      "Cost after iteration 100000: 2.876397\n",
      "Cost after iteration 110000: 2.870566\n",
      "The accuracy on each of the folds is: [51.300000000000004, 48.0, 48.5, 53.0]\n",
      "The mean accuracy of my model is: 50.2\n"
     ]
    }
   ],
   "source": [
    "accuracy_avg = []\n",
    "for i, fold in enumerate(folds): #loop through each fold\n",
    "    training_set = np.delete(folds, i, axis=0).reshape(8, 250*3)\n",
    "    parameters = nn_model(training_set, training_set, 3, 120000, print_cost=True)\n",
    "    predicted = predict(parameters, folds[i])\n",
    "    accuracy = accuracy_metric(folds[i], predicted, 0.15)\n",
    "    accuracy_avg.append(accuracy)\n",
    "print(\"The accuracy on each of the folds is:\",accuracy_avg)\n",
    "print(\"The mean accuracy of my model is:\",np.mean(accuracy_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy of ~50% is more acceptable to me, although not great. I think I could improve this more if I was willing to run more epochs.\n",
    "\n",
    "However, I now realize that I misunderstood the instructions--it wants me to be able to reconstruct an 8x8 IDENTITY MATRIX, not necessarily run this extra evaluation. I will see how my neural net performs on an identity matrix after being trained on 120,000 epochs and my autoencoder_X_int training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.545267\n",
      "Cost after iteration 10000: 3.162339\n",
      "Cost after iteration 20000: 3.061024\n",
      "Cost after iteration 30000: 3.010675\n",
      "Cost after iteration 40000: 2.978036\n",
      "Cost after iteration 50000: 2.954299\n",
      "Cost after iteration 60000: 2.935858\n",
      "Cost after iteration 70000: 2.920898\n",
      "Cost after iteration 80000: 2.908385\n",
      "Cost after iteration 90000: 2.898364\n",
      "Cost after iteration 100000: 2.891053\n",
      "Cost after iteration 110000: 2.884528\n"
     ]
    }
   ],
   "source": [
    "final_autoencoder_params = nn_model(autoencoder_X_int, autoencoder_X_int, 3, 120000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "identity_matrix = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                           [0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "print(identity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder thinks the answer is: [[0.99998987]\n",
      " [0.53329991]\n",
      " [1.        ]\n",
      " [0.52774607]\n",
      " [0.06069324]\n",
      " [0.4791376 ]\n",
      " [1.        ]\n",
      " [0.00526253]]\n",
      "The ground-truth is: [[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "answers = predict(final_autoencoder_params, test_int)\n",
    "print(\"Autoencoder thinks the answer is:\",answers)\n",
    "print(\"The ground-truth is:\",test_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder thinks the answer is: [[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "identity_matrix_predicted = predict(final_autoencoder_params, identity_matrix[0])\n",
    "print(\"Autoencoder thinks the answer is:\",identity_matrix_predicted.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 4])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [1, 3, 2, 4]\n",
    "np.delete(test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are attempts to preprocess my positive training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathway = os.path.join('./data', 'rap1-lieb-positives.txt')\n",
    "train = np.loadtxt(os.path.abspath(pathway), dtype=str)\n",
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###one-hot encoding all training examples and flattening into matrix of shape (length of sequence*length of onehot vector, m)\n",
    "\n",
    "\n",
    "# define universe of possible input values\n",
    "alphabet = 'ACTG'\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "\n",
    "#X = np.zeros((17*4, len(train)))\n",
    "X = []\n",
    "for sequence in train:\n",
    "    # integer encode input data\n",
    "    integer_encoding = [char_to_int[char] for char in sequence]\n",
    "    # one hot encode\n",
    "    onehot_encoding = []\n",
    "    for value in integer_encoding:\n",
    "        letter = [0 for _ in range(len(alphabet))]\n",
    "        letter[value] = 1\n",
    "        onehot_encoding.append(letter)\n",
    "    final_onehot = list(itertools.chain(*onehot_encoding))\n",
    "    X.append(final_onehot)\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(X[:,5].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X #for an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parameters = nn_model(X, Y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(optimized_parameters, X[:,5]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
